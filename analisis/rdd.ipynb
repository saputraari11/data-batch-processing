{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pq.ParquetFile(\"./data/pq/green/2019/01/part-00000-a9762b67-85ff-4157-8b2c-ec9814daf9a4-c000.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_parque = df.num_row_groups\n",
    "print(table_parque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in range(table_parque):\n",
    "#     x = df.read_row_groups(row,use_pandas_metadata=True).to_pandas()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:48:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/20 12:48:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.app.name', 'into_rdd'),\n",
       " ('spark.app.startTime', '1676872093757'),\n",
       " ('spark.driver.port', '32921'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', 'sapautraArch'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/saputra/belajar/data-batch-processing/analisis/spark-warehouse'),\n",
       " ('spark.app.submitTime', '1676872093673'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.id', 'local-1676872094513')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\").setAppName(\"into_rdd\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sc.defaultParallelism\n",
    "spark = SQLContext(sc)\n",
    "spark.setConf(\"spark.default.parallelism\", \"100\")\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "SELECT \n",
    "    -- Reveneue grouping \n",
    "    PULocationID AS zone,\n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "\n",
    "    -- Revenue sum\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "-- ORDER BY \n",
    "--     1, 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.parquet('./data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet('./data/pq/yellow/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2019, 3, 1, 21, 17, 13), tpep_dropoff_datetime=datetime.datetime(2019, 3, 1, 21, 23, 3), passenger_count=1, trip_distance=0.68, RatecodeID=1, store_and_fwd_flag='N', PULocationID=114, DOLocationID=79, payment_type=1, fare_amount=5.5, extra=0.5, mta_tax=0.5, tip_amount=1.86, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=11.16, congestion_surcharge=2.5),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2019, 3, 14, 8, 30, 52), tpep_dropoff_datetime=datetime.datetime(2019, 3, 14, 8, 36, 11), passenger_count=1, trip_distance=0.84, RatecodeID=1, store_and_fwd_flag='N', PULocationID=48, DOLocationID=161, payment_type=1, fare_amount=5.5, extra=0.0, mta_tax=0.5, tip_amount=1.2, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=10.0, congestion_surcharge=2.5),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2019, 3, 16, 17, 51, 8), tpep_dropoff_datetime=datetime.datetime(2019, 3, 16, 17, 59, 55), passenger_count=2, trip_distance=1.11, RatecodeID=1, store_and_fwd_flag='N', PULocationID=79, DOLocationID=234, payment_type=1, fare_amount=7.0, extra=0.0, mta_tax=0.5, tip_amount=2.06, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=12.36, congestion_surcharge=2.5),\n",
       " Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2019, 3, 26, 18, 42, 15), tpep_dropoff_datetime=datetime.datetime(2019, 3, 26, 18, 56, 50), passenger_count=1, trip_distance=2.6, RatecodeID=1, store_and_fwd_flag='N', PULocationID=262, DOLocationID=142, payment_type=1, fare_amount=11.5, extra=3.5, mta_tax=0.5, tip_amount=3.15, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=18.95, congestion_surcharge=2.5),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2019, 3, 29, 21, 42, 9), tpep_dropoff_datetime=datetime.datetime(2019, 3, 29, 21, 50, 3), passenger_count=1, trip_distance=1.83, RatecodeID=1, store_and_fwd_flag='N', PULocationID=263, DOLocationID=238, payment_type=1, fare_amount=8.5, extra=0.5, mta_tax=0.5, tip_amount=2.46, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=14.76, congestion_surcharge=2.5)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yellow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_yellow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rdd \u001b[39m=\u001b[39m df_yellow\u001b[39m.\u001b[39mselect(\u001b[39m'\u001b[39m\u001b[39mtpep_pickup_datetime\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mPULocationID\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtotal_amount\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mrdd\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_yellow' is not defined"
     ]
    }
   ],
   "source": [
    "rdd = df_yellow.select('tpep_pickup_datetime','PULocationID','total_amount').rdd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where Clause in rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.filter(lambda row: True).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2021,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_outliers(row):\n",
    "    return row.tpep_pickup_datetime >= start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.filter(filter_outliers).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = rdd.take(10)\n",
    "row = rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group BY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Grouping is create new rdd with structure data is (key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareForGrouping(row):\n",
    "    hour = row.tpep_pickup_datetime.replace(minute=0,second=0,microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "\n",
    "    count = 1\n",
    "    amount = row.total_amount\n",
    "    \n",
    "    key = (hour,zone)\n",
    "    value = (amount,count)\n",
    "    return (key,value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tranform Value is reduce or get row value before and current value, \n",
    "#### for example old is v1 and current is v2 and return value like tuple data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranformValue(oldValue,curValue):\n",
    "    oldAmount,oldCount = oldValue\n",
    "    curAmount,curCount = curValue\n",
    "\n",
    "    totalAmount = oldAmount + curAmount\n",
    "    totalCount = oldCount + curCount\n",
    "    return (totalAmount,totalCount)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unwrap is function for create new structure to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unWrap(row):\n",
    "    return (row[0][1],row[0][0],row[1][0],row[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_schema = StructType([\n",
    "    StructField(\"zone\",LongType(),True),\n",
    "    StructField(\"datetime\",TimestampType(),True),\n",
    "    StructField(\"amount\",DoubleType(),True),\n",
    "    StructField(\"total_transaction\",LongType(),True),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new structure and convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.filter(filter_outliers).map(prepareForGrouping).reduceByKey(tranformValue).map(unWrap).toDF(rdd_schema).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Mechine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['VendorID', 'lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance']\n",
    "\n",
    "duration_rdd = df_green \\\n",
    "    .select(columns) \\\n",
    "    .rdd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(df):\n",
    "    y_pred = df.trip_distance * 5\n",
    "    return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "#### Iterate Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_in_batch(partition):\n",
    "    df = pd.DataFrame(partition,columns=columns)\n",
    "    predictions = model_predict(df)\n",
    "    df[\"prediction_duration\"] = predictions\n",
    "    return df.itertuples()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_predict = duration_rdd.mapPartitions(apply_model_in_batch).toDF().drop('Index')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mechine Learning ready to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|prediction_duration|\n",
      "+-------------------+\n",
      "|                6.0|\n",
      "|               50.0|\n",
      "| 28.900000000000002|\n",
      "|               5.25|\n",
      "|  7.800000000000001|\n",
      "|              13.95|\n",
      "|                4.8|\n",
      "|                7.5|\n",
      "|               25.8|\n",
      "|               11.7|\n",
      "|               4.25|\n",
      "|               12.0|\n",
      "|              50.05|\n",
      "|                8.6|\n",
      "|                5.2|\n",
      "|                2.5|\n",
      "|              100.7|\n",
      "|               25.0|\n",
      "| 106.44999999999999|\n",
      "|                0.0|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_predict.select(\"prediction_duration\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate Partition and play with chunk data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick one partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rowbe=duration_rdd.glom().collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rowbe,columns=columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create chunk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_loop():\n",
    "    return df.itertuples()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate Chunk Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_chunk = infinite_loop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call chunk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas(Index=8, VendorID=2, lpep_pickup_datetime=Timestamp('2019-01-23 16:34:55'), PULocationID=136, DOLocationID=42, trip_distance=5.16)\n"
     ]
    }
   ],
   "source": [
    "data = next(process_chunk)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in process_chunk:\n",
    "    print(i)\n",
    "\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "pool = ThreadPool(8)\n",
    "pool.map(apply_model_in_batch,range(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10 (main, Mar  5 2023, 22:26:53) [GCC 12.2.1 20230201]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
