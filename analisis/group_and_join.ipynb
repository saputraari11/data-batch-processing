{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f4504992eb0>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\").setAppName(\"into_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:833)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2661)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2658)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2748)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:97)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(conf)\n",
      "File \u001b[0;32m~/belajar/data-batch-processing/belajar_batch/lib/python3.9/site-packages/pyspark/context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    482\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[1;32m    484\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/belajar/data-batch-processing/belajar_batch/lib/python3.9/site-packages/pyspark/context.py:197\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m SparkContext\u001b[39m.\u001b[39m_ensure_initialized(\u001b[39mself\u001b[39m, gateway\u001b[39m=\u001b[39mgateway, conf\u001b[39m=\u001b[39mconf)\n\u001b[1;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[1;32m    200\u001b[0m         sparkHome,\n\u001b[1;32m    201\u001b[0m         pyFiles,\n\u001b[1;32m    202\u001b[0m         environment,\n\u001b[1;32m    203\u001b[0m         batchSize,\n\u001b[1;32m    204\u001b[0m         serializer,\n\u001b[1;32m    205\u001b[0m         conf,\n\u001b[1;32m    206\u001b[0m         jsc,\n\u001b[1;32m    207\u001b[0m         profiler_cls,\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    210\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[39m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/belajar/data-batch-processing/belajar_batch/lib/python3.9/site-packages/pyspark/context.py:282\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment[\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[39m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc \u001b[39m=\u001b[39m jsc \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_context(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conf\u001b[39m.\u001b[39;49m_jconf)\n\u001b[1;32m    283\u001b[0m \u001b[39m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m SparkConf(_jconf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39mconf())\n",
      "File \u001b[0;32m~/belajar/data-batch-processing/belajar_batch/lib/python3.9/site-packages/pyspark/context.py:402\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mJavaSparkContext(jconf)\n",
      "File \u001b[0;32m~/belajar/data-batch-processing/belajar_batch/lib/python3.9/site-packages/py4j/java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1579\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1580\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1581\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1584\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1585\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1586\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[1;32m   1588\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1589\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/belajar/data-batch-processing/belajar_batch/lib/python3.9/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/belajar/data-batch-processing/belajar_batch/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:833)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2661)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2658)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2748)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:97)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 10:53:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saputra/belajar/data-batch-processing/belajar_batch/lib/python3.9/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "spark = SQLContext(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GROUP Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.option(\"header\",True).parquet(\"./data/pq/green/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saputra/belajar/data-batch-processing/belajar_batch/lib/python3.9/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_green.registerTempTable(\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    -- Reveneue grouping \n",
    "    PULocationID AS zone,\n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "\n",
    "    -- Revenue sum\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "-- ORDER BY \n",
    "--     1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==================================================>      (22 + 3) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------------+--------------+\n",
      "|zone|               hour|            amount|number_records|\n",
      "+----+-------------------+------------------+--------------+\n",
      "| 260|2020-01-01 02:00:00|235.16999999999996|            20|\n",
      "| 260|2020-01-01 07:00:00|             67.25|             5|\n",
      "|  25|2020-01-02 15:00:00| 749.4300000000001|            29|\n",
      "|  83|2020-01-05 22:00:00|             59.66|             5|\n",
      "|  41|2020-01-16 10:00:00| 549.0500000000001|            42|\n",
      "| 116|2020-01-13 09:00:00|            583.07|            29|\n",
      "|  97|2020-01-17 17:00:00| 783.5600000000002|            50|\n",
      "|  74|2020-01-07 03:00:00|               7.8|             1|\n",
      "|   3|2020-01-06 12:00:00|            122.49|             6|\n",
      "|  82|2020-01-11 01:00:00|            501.49|            42|\n",
      "| 254|2020-01-05 18:00:00|43.120000000000005|             2|\n",
      "|  65|2020-01-28 16:00:00| 702.1600000000001|            28|\n",
      "|  38|2020-01-31 11:00:00|            105.58|             3|\n",
      "|  33|2020-01-25 16:00:00|            242.14|            17|\n",
      "|  42|2020-01-10 12:00:00|349.37000000000006|            26|\n",
      "|  74|2020-01-02 17:00:00| 799.9000000000001|            54|\n",
      "|  74|2020-01-25 22:00:00| 513.1800000000001|            44|\n",
      "| 166|2020-01-29 19:00:00| 695.0100000000002|            45|\n",
      "|  35|2020-01-09 11:00:00|             88.57|             3|\n",
      "| 181|2020-01-23 22:00:00|            903.41|            32|\n",
      "+----+-------------------+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================>                    (16 + 9) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 10:53:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/02/14 10:53:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/14 10:53:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/02/14 10:53:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/02/14 10:53:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 10:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/02/14 10:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/02/14 10:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/14 10:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green.write.parquet(\"./data/report/revenue/green\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_yellow = spark.read.option(\"header\",True).parquet(\"./data/pq/yellow/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.dropTempTable(\"yellowNew\")\n",
    "# df_yellow = spark.sql(\"\"\"\n",
    "#     DROP VIEW IF EXISTS yellowNew\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saputra/belajar/data-batch-processing/belajar_batch/lib/python3.9/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_yellow.registerTempTable(\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    -- Reveneue grouping \n",
    "    PULocationID AS zone,\n",
    "    date_trunc('hour', tpep_pickup_datetime) AS hour, \n",
    "\n",
    "    -- Revenue sum\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    yellow\n",
    "WHERE\n",
    "    tpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "-- ORDER BY \n",
    "--     1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                       (0 + 12) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/02/14 10:56:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow.repartition(20).write.parquet(\"./data/report/revenue/yellow\",mode=\"overwrite\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOIN Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.option(\"header\",True).parquet(\"./data/report/revenue/yellow/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------------+--------------+\n",
      "|zone|               hour|            amount|number_records|\n",
      "+----+-------------------+------------------+--------------+\n",
      "|  48|2021-01-07 16:00:00|1330.5100000000002|            85|\n",
      "| 201|2021-02-03 10:00:00|             67.09|             1|\n",
      "| 129|2020-11-28 18:00:00|             67.15|             3|\n",
      "|  76|2021-01-18 14:00:00|              83.2|             3|\n",
      "|  86|2020-08-26 04:00:00|              36.0|             1|\n",
      "|  25|2021-05-17 15:00:00|             21.72|             2|\n",
      "| 234|2021-03-19 17:00:00|2820.4300000000003|           158|\n",
      "| 243|2020-10-20 08:00:00|              12.3|             1|\n",
      "| 113|2020-06-26 15:00:00|219.46000000000004|            15|\n",
      "| 211|2020-10-20 00:00:00|             63.75|             4|\n",
      "| 132|2020-02-16 08:00:00| 8785.680000000004|           181|\n",
      "| 186|2020-02-03 01:00:00|1341.3799999999997|            69|\n",
      "|  13|2020-07-07 09:00:00|             92.22|             5|\n",
      "| 133|2020-10-02 12:00:00|              12.3|             1|\n",
      "| 265|2020-04-28 06:00:00|             33.19|             1|\n",
      "| 262|2020-07-23 09:00:00| 830.0000000000002|            56|\n",
      "| 219|2020-01-04 15:00:00|              55.3|             1|\n",
      "| 170|2020-07-09 11:00:00|1047.1100000000001|            70|\n",
      "| 234|2020-11-17 22:00:00| 725.5600000000001|            44|\n",
      "| 107|2021-04-05 05:00:00|             63.31|             5|\n",
      "+----+-------------------+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yellow.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.option(\"header\",True).parquet(\"./data/report/revenue/green/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------------+--------------+\n",
      "|zone|               hour|            amount|number_records|\n",
      "+----+-------------------+------------------+--------------+\n",
      "| 226|2020-03-07 09:00:00|             73.33|             5|\n",
      "|  75|2020-01-28 19:00:00|1299.0000000000002|            97|\n",
      "|  65|2020-01-30 00:00:00|             75.37|             6|\n",
      "| 181|2020-01-25 20:00:00|            366.33|            26|\n",
      "| 129|2020-01-26 04:00:00| 497.1500000000001|            39|\n",
      "|  25|2020-01-13 18:00:00|            215.32|            16|\n",
      "|  80|2020-01-22 11:00:00|             57.18|             4|\n",
      "|  65|2020-01-07 20:00:00|            349.12|            22|\n",
      "|  41|2020-01-09 09:00:00| 955.1400000000002|            69|\n",
      "| 129|2020-01-15 10:00:00|221.47000000000003|            12|\n",
      "| 135|2020-01-08 08:00:00|            112.88|             4|\n",
      "| 228|2020-01-15 18:00:00|             35.91|             2|\n",
      "| 213|2020-01-08 07:00:00|            342.44|            12|\n",
      "| 244|2020-01-17 11:00:00|            588.57|            25|\n",
      "| 226|2020-01-01 16:00:00|            142.74|             6|\n",
      "|  17|2020-01-11 21:00:00|             50.41|             3|\n",
      "| 259|2020-01-06 09:00:00|            288.14|            12|\n",
      "| 212|2020-01-22 19:00:00|            117.18|             4|\n",
      "| 174|2020-01-06 12:00:00|            197.03|             9|\n",
      "| 260|2020-01-24 20:00:00|            354.19|            20|\n",
      "+----+-------------------+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = df_green.withColumnRenamed(\"amount\",\"green_amount\").withColumnRenamed(\"number_records\",\"green_number_records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = df_yellow.withColumnRenamed(\"amount\",\"yellow_amount\").withColumnRenamed(\"number_records\",\"yellow_number_records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_green.join(df_yellow,[\"zone\",\"hour\"],how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 11:07:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/02/14 11:07:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/14 11:07:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/02/14 11:07:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/02/14 11:07:45 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                       (0 + 12) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 11:07:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/02/14 11:07:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/02/14 11:07:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/14 11:07:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join.write.parquet(\"./data/report/revenue/total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = spark.read.option(\"header\",True).parquet(\"./data/report/revenue/total/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------+--------------------+------------------+---------------------+\n",
      "|zone|               hour|green_amount|green_number_records|     yellow_amount|yellow_number_records|\n",
      "+----+-------------------+------------+--------------------+------------------+---------------------+\n",
      "|   1|2020-01-01 09:00:00|        null|                null|            175.26|                    2|\n",
      "|   1|2020-01-02 14:00:00|        null|                null|            628.52|                    6|\n",
      "|   1|2020-01-02 21:00:00|        null|                null|             100.3|                    1|\n",
      "|   1|2020-01-04 05:00:00|        null|                null|              45.8|                    1|\n",
      "|   1|2020-01-06 16:00:00|        null|                null|             165.6|                    2|\n",
      "|   1|2020-01-07 15:00:00|        null|                null|            117.96|                    1|\n",
      "|   1|2020-01-08 02:00:00|        null|                null|             24.36|                    1|\n",
      "|   1|2020-01-08 16:00:00|        null|                null|            341.76|                    3|\n",
      "|   1|2020-01-12 15:00:00|        null|                null|            247.62|                    2|\n",
      "|   1|2020-01-12 23:00:00|        null|                null| 4.109999999999999|                    3|\n",
      "|   1|2020-01-13 08:00:00|        null|                null|             222.8|                    2|\n",
      "|   1|2020-01-14 05:00:00|        null|                null|              80.3|                    1|\n",
      "|   1|2020-01-17 19:00:00|        null|                null|            566.67|                    5|\n",
      "|   1|2020-01-18 07:00:00|        null|                null|            168.35|                    1|\n",
      "|   1|2020-01-19 06:00:00|        null|                null|             82.05|                    1|\n",
      "|   1|2020-01-19 15:00:00|        null|                null|              75.3|                    1|\n",
      "|   1|2020-01-21 05:00:00|        null|                null|              23.3|                    1|\n",
      "|   1|2020-01-22 08:00:00|        null|                null|            108.36|                    1|\n",
      "|   1|2020-01-22 15:00:00|        null|                null|347.46000000000004|                    5|\n",
      "|   1|2020-01-23 10:00:00|        null|                null|              96.3|                    1|\n",
      "+----+-------------------+------------+--------------------+------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------------+--------------------+-------------+---------------------+\n",
      "|zone|               hour|      green_amount|green_number_records|yellow_amount|yellow_number_records|\n",
      "+----+-------------------+------------------+--------------------+-------------+---------------------+\n",
      "|   1|2020-02-06 07:00:00|              98.3|                   1|        82.61|                    2|\n",
      "|   1|2020-03-21 17:00:00|             100.3|                   1|         null|                 null|\n",
      "|   1|2020-10-15 18:00:00|            126.79|                   1|         null|                 null|\n",
      "|   1|2020-11-10 08:00:00|             101.3|                   1|         null|                 null|\n",
      "|   1|2021-04-12 07:00:00|              92.8|                   1|         null|                 null|\n",
      "|   3|2020-01-02 18:00:00|              16.8|                   1|         null|                 null|\n",
      "|   3|2020-01-03 11:00:00|25.470000000000002|                   2|         null|                 null|\n",
      "|   3|2020-01-03 13:00:00|             43.12|                   1|         null|                 null|\n",
      "|   3|2020-01-06 08:00:00|             74.92|                   2|         null|                 null|\n",
      "|   3|2020-01-06 09:00:00|127.71000000000001|                   4|         null|                 null|\n",
      "|   3|2020-01-09 13:00:00|             64.01|                   2|         null|                 null|\n",
      "|   3|2020-01-10 10:00:00|             48.99|                   2|         null|                 null|\n",
      "|   3|2020-01-15 08:00:00|             21.64|                   1|         null|                 null|\n",
      "|   3|2020-01-16 07:00:00|             45.24|                   1|        37.74|                    1|\n",
      "|   3|2020-01-17 10:00:00|               9.3|                   1|         null|                 null|\n",
      "|   3|2020-01-19 21:00:00|             16.67|                   1|         null|                 null|\n",
      "|   3|2020-01-20 23:00:00|             30.32|                   1|         null|                 null|\n",
      "|   3|2020-01-21 21:00:00|             50.45|                   1|         null|                 null|\n",
      "|   3|2020-01-22 11:00:00|             45.24|                   1|         null|                 null|\n",
      "|   3|2020-01-22 17:00:00|             30.32|                   1|         null|                 null|\n",
      "+----+-------------------+------------------+--------------------+-------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.filter(df_join[\"green_amount\"].isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------+--------------------+------------------+---------------------+\n",
      "|zone|               hour|green_amount|green_number_records|     yellow_amount|yellow_number_records|\n",
      "+----+-------------------+------------+--------------------+------------------+---------------------+\n",
      "|   1|2020-01-01 09:00:00|        null|                null|            175.26|                    2|\n",
      "|   1|2020-01-02 14:00:00|        null|                null|            628.52|                    6|\n",
      "|   1|2020-01-02 21:00:00|        null|                null|             100.3|                    1|\n",
      "|   1|2020-01-04 05:00:00|        null|                null|              45.8|                    1|\n",
      "|   1|2020-01-06 16:00:00|        null|                null|             165.6|                    2|\n",
      "|   1|2020-01-07 15:00:00|        null|                null|            117.96|                    1|\n",
      "|   1|2020-01-08 02:00:00|        null|                null|             24.36|                    1|\n",
      "|   1|2020-01-08 16:00:00|        null|                null|            341.76|                    3|\n",
      "|   1|2020-01-12 15:00:00|        null|                null|            247.62|                    2|\n",
      "|   1|2020-01-12 23:00:00|        null|                null| 4.109999999999999|                    3|\n",
      "|   1|2020-01-13 08:00:00|        null|                null|             222.8|                    2|\n",
      "|   1|2020-01-14 05:00:00|        null|                null|              80.3|                    1|\n",
      "|   1|2020-01-17 19:00:00|        null|                null|            566.67|                    5|\n",
      "|   1|2020-01-18 07:00:00|        null|                null|            168.35|                    1|\n",
      "|   1|2020-01-19 06:00:00|        null|                null|             82.05|                    1|\n",
      "|   1|2020-01-19 15:00:00|        null|                null|              75.3|                    1|\n",
      "|   1|2020-01-21 05:00:00|        null|                null|              23.3|                    1|\n",
      "|   1|2020-01-22 08:00:00|        null|                null|            108.36|                    1|\n",
      "|   1|2020-01-22 15:00:00|        null|                null|347.46000000000004|                    5|\n",
      "|   1|2020-01-23 10:00:00|        null|                null|              96.3|                    1|\n",
      "+----+-------------------+------------+--------------------+------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.filter(df_join[\"yellow_amount\"].isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------+--------------------+------------------+---------------------+\n",
      "|zone|               hour|green_amount|green_number_records|     yellow_amount|yellow_number_records|\n",
      "+----+-------------------+------------+--------------------+------------------+---------------------+\n",
      "|   1|2020-02-06 07:00:00|        98.3|                   1|             82.61|                    2|\n",
      "|   3|2020-01-16 07:00:00|       45.24|                   1|             37.74|                    1|\n",
      "|   3|2020-01-27 08:00:00|       55.04|                   3|             33.85|                    1|\n",
      "|   3|2020-01-28 07:00:00|       45.24|                   1|              29.0|                    1|\n",
      "|   3|2020-01-31 15:00:00|       30.48|                   1|              40.8|                    1|\n",
      "|   3|2020-02-09 16:00:00|      163.82|                   1|             35.28|                    1|\n",
      "|   3|2020-02-10 11:00:00|       53.22|                   1|              36.0|                    1|\n",
      "|   3|2020-02-12 15:00:00|       30.48|                   1|              16.0|                    1|\n",
      "|   3|2020-02-13 07:00:00|       21.64|                   1|             26.48|                    1|\n",
      "|   3|2020-02-15 16:00:00|       16.67|                   1|             42.67|                    1|\n",
      "|   3|2020-02-25 06:00:00|       30.32|                   1|             55.11|                    1|\n",
      "|   3|2020-02-28 07:00:00|       16.67|                   1|             16.67|                    1|\n",
      "|   3|2020-03-07 10:00:00|        23.0|                   2|             46.11|                    3|\n",
      "|   3|2020-03-08 10:00:00|         8.3|                   1|             47.75|                    2|\n",
      "|   3|2020-03-10 15:00:00|        9.15|                   1|               7.8|                    1|\n",
      "|   3|2020-04-27 16:00:00|       54.11|                   1|             12.36|                    1|\n",
      "|   3|2020-05-09 08:00:00|       48.19|                   1|52.989999999999995|                    2|\n",
      "|   3|2020-05-18 07:00:00|       27.93|                   1|              6.24|                    1|\n",
      "|   3|2020-08-07 15:00:00|       23.85|                   1|             22.01|                    1|\n",
      "|   3|2020-08-11 03:00:00|        60.0|                   1|              65.4|                    1|\n",
      "+----+-------------------+------------+--------------------+------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.filter((df_join[\"yellow_amount\"].isNotNull() & df_join[\"green_amount\"].isNotNull())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone = spark.read.option(\"header\",True).parquet(\"./data/pq/zone/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_join.join(df_zone,df_zone.LocationID == df_join.zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------+--------------------+------------------+---------------------+----------+-------+--------------+------------+\n",
      "|zone|               hour|green_amount|green_number_records|     yellow_amount|yellow_number_records|LocationID|Borough|     zone_name|service_zone|\n",
      "+----+-------------------+------------+--------------------+------------------+---------------------+----------+-------+--------------+------------+\n",
      "|   1|2020-01-01 09:00:00|        null|                null|            175.26|                    2|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-02 14:00:00|        null|                null|            628.52|                    6|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-02 21:00:00|        null|                null|             100.3|                    1|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-04 05:00:00|        null|                null|              45.8|                    1|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-06 16:00:00|        null|                null|             165.6|                    2|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-07 15:00:00|        null|                null|            117.96|                    1|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-08 02:00:00|        null|                null|             24.36|                    1|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-08 16:00:00|        null|                null|            341.76|                    3|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-12 15:00:00|        null|                null|            247.62|                    2|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-12 23:00:00|        null|                null| 4.109999999999999|                    3|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-13 08:00:00|        null|                null|             222.8|                    2|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-14 05:00:00|        null|                null|              80.3|                    1|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-17 19:00:00|        null|                null|            566.67|                    5|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-18 07:00:00|        null|                null|            168.35|                    1|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-19 06:00:00|        null|                null|             82.05|                    1|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-19 15:00:00|        null|                null|              75.3|                    1|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-21 05:00:00|        null|                null|              23.3|                    1|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-22 08:00:00|        null|                null|            108.36|                    1|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-22 15:00:00|        null|                null|347.46000000000004|                    5|         1|    EWR|Newark Airport|         EWR|\n",
      "|   1|2020-01-23 10:00:00|        null|                null|              96.3|                    1|         1|    EWR|Newark Airport|         EWR|\n",
      "+----+-------------------+------------+--------------------+------------------+---------------------+----------+-------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 12:03:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/02/14 12:03:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/14 12:03:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/02/14 12:03:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/02/14 12:03:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 108:>                                                      (0 + 12) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 12:03:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/02/14 12:03:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/02/14 12:03:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/14 12:03:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/02/14 12:03:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/02/14 12:03:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.drop(df_result.LocationID).write.parquet(\"./data/temp/revenue-zones\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "belajar_batch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8cc084d3f3e07bb8571b2a078dca074b14c05ca679100a9a371d8e778c10801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
